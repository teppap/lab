# -*- coding: utf-8 -*-
"""5.RF

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y5-sMPwOg1R4YiYdYTQB5HKccokQOo1y
"""

# ==============================================================================
# ส่วนที่ 1: ติดตั้งและโหลด Library ที่จำเป็น
# ==============================================================================
print("ทำการติดตั้ง library ที่จำเป็น...")
!pip install datasets scikit-learn -q

import pandas as pd
from datasets import load_dataset
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, accuracy_score
import joblib

print("ติดตั้งและโหลด library เรียบร้อยแล้ว!\n")

# ==============================================================================
# ส่วนที่ 2: โหลดและสำรวจชุดข้อมูล (Dataset)
# ==============================================================================
# คำอธิบาย:
# เราจะใช้ library `datasets` เพื่อโหลดชุดข้อมูล 'shengqin/web-attacks-old'
# ชุดข้อมูลนี้ประกอบด้วยข้อมูล traffic ของเว็บไซต์ ซึ่งถูกแบ่งเป็น 'Normal' (ปกติ)
# และประเภทการโจมตีต่างๆ เช่น 'SQL Injection', 'XSS'

print("กำลังโหลดชุดข้อมูล 'shengqin/web-attacks-old' จาก Hugging Face Hub...")
# โหลดชุดข้อมูลทั้งหมด
full_dataset = load_dataset("shengqin/web-attacks-old")

# แปลงชุดข้อมูล train เป็น Pandas DataFrame เพื่อให้ง่ายต่อการจัดการ
df = full_dataset['train'].to_pandas()
print("โหลดชุดข้อมูลสำเร็จแล้ว!\n")

# --- แสดงข้อมูลเบื้องต้น ---
print("="*50)
print("ข้อมูลเบื้องต้นของ Dataset (5 แถวแรก):")
print(df.head())
print("\n" + "="*50)

print("\nขนาดของข้อมูล:")
print(f"จำนวนแถว: {df.shape[0]}, จำนวนคอลัมน์: {df.shape[1]}")
print("\n" + "="*50)

print("\nประเภทข้อมูลในแต่ละคอลัมน์:")
print(df.info())
print("\n" + "="*50)

print("\nการกระจายตัวของประเภทการโจมตี (Label):")
print(df['text_label'].value_counts())
print("="*50)

# กำหนดให้ X คือข้อมูล 'Payload' ที่เป็นข้อความดิบ
# กำหนดให้ y คือ 'Label' ที่เป็นตัวเลข (0, 1, 2, ...)
X = df['Payload']
y = df['Label']

print("โหลดและเตรียมข้อมูลสำเร็จ!\n")
print(f"ตัวอย่างข้อมูล (X):\n{X.head()}\n")
print(f"ตัวอย่างคำตอบ (y):\n{y.head()}\n")

# ==============================================================================
# ส่วนที่ 3: แบ่งข้อมูลสำหรับ Train และ Test
# ==============================================================================
# คำอธิบาย:
# แบ่งข้อมูล 80% ไว้สำหรับฝึกสอน (Train) และ 20% ไว้สำหรับทดสอบ (Test)
# เพื่อให้เราสามารถวัดผลโมเดลกับข้อมูลที่มันไม่เคยเห็นมาก่อนได้

print("="*50)
print("กำลังแบ่งข้อมูลเป็นชุดสำหรับ Train (80%) และ Test (20%)...")
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,    # สัดส่วนข้อมูล test
    random_state=42,  # ทำให้ผลการสุ่มเหมือนเดิมทุกครั้งที่รัน
    stratify=y        # รักษาสัดส่วนของแต่ละ label ให้เท่ากันในชุด train/test
)
print("แบ่งข้อมูลเรียบร้อย!\n")

# ==============================================================================
# ส่วนที่ 4: สร้าง, เทรน, และบันทึก Pipeline
# ==============================================================================
# คำอธิบาย:
# เราจะสร้าง 'Pipeline' ซึ่งเป็นท่อที่รวมขั้นตอนการทำงาน 2 อย่างไว้ด้วยกันคือ:
#  1. TfidfVectorizer: แปลงข้อความดิบ (ประโยค) ให้เป็นชุดตัวเลขเวกเตอร์
#     ที่โมเดลคอมพิวเตอร์สามารถเข้าใจได้
#  2. RandomForestClassifier: โมเดลจำแนกประเภทที่เรียนรู้จากตัวเลขเวกเตอร์นั้น
# การใช้ Pipeline ทำให้โค้ดสะอาดและลดโอกาสเกิดข้อผิดพลาด

print("="*50)
print("กำลังสร้าง Pipeline (TF-IDF + RandomForest)...")

# สร้าง Pipeline
web_attack_pipeline = Pipeline([
    # ให้ Vectorizer มองเป็นกลุ่มตัวอักษร 3-5 ตัวที่ติดกัน
    ('tfidf', TfidfVectorizer(analyzer='char', ngram_range=(3, 5), max_features=10000)),
    ('clf', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1))
])

# เริ่มเทรน Pipeline ทั้งระบบด้วยข้อมูล Training Set
print("กำลังเทรน Pipeline...")
web_attack_pipeline.fit(X_train, y_train)
print("เทรน Pipeline สำเร็จแล้ว!")


# --- การตั้งชื่อและบันทึก Pipeline ---
# คำอธิบาย:
# หลังจากเทรนเสร็จ เราจะบันทึก Pipeline ทั้งหมด (ทั้งตัวแปลงข้อความและโมเดล)
# ลงในไฟล์เดียว เพื่อให้สามารถนำไปใช้ในอนาคตได้ทันที
print("\nกำลังบันทึก Pipeline ลงไฟล์...")
pipeline_filename = 'web_attack_pipeline.joblib'
joblib.dump(web_attack_pipeline, pipeline_filename)
print(f"✅ บันทึก Pipeline สำเร็จในชื่อ: '{pipeline_filename}'\n")

# ==============================================================================
# ส่วนที่ 5: ประเมินประสิทธิภาพของ Pipeline ที่เทรนแล้ว
# ==============================================================================
# คำอธิบาย:
# เราจะใช้ข้อมูล Test Set ที่แบ่งไว้ มาทดสอบประสิทธิภาพของ Pipeline
# เพื่อดูว่ามันทำงานได้ดีแค่ไหนกับข้อมูลที่มันไม่เคยเห็นมาก่อน

print("="*50)
print("กำลังประเมินผล Pipeline ด้วย Test Set...")

# ใช้ Pipeline ทำนายผลจากข้อมูล Test
y_pred = web_attack_pipeline.predict(X_test)

# แสดงผลลัพธ์การประเมิน
accuracy = accuracy_score(y_test, y_pred)
print(f"\nความแม่นยำ (Accuracy): {accuracy:.4f} หรือ {accuracy*100:.2f}%\n")

print("--- Classification Report ---")
# สร้าง mapping จาก label (ตัวเลข) กลับไปเป็นชื่อ (ตัวอักษร) เพื่อให้อ่าน report ง่ายขึ้น
target_names_map = df.set_index('Label')['text_label'].to_dict()
target_names_list = [target_names_map[i] for i in sorted(target_names_map.keys())]
print(classification_report(y_test, y_pred, target_names=target_names_list))
print("="*50)

# ==============================================================================
# ส่วนที่ 1: ติดตั้ง Library และเตรียม Environment
# ==============================================================================
print("ติดตั้ง library ที่จำเป็น (scikit-learn, joblib)...")
!pip install scikit-learn -q

import joblib
import pandas as pd
import os

print("ติดตั้งและ import library เรียบร้อย!\n")


# ==============================================================================
# ส่วนที่ 2: โหลด Pipeline และส่วนประกอบที่จำเป็น
# ==============================================================================
# คำอธิบาย:
# นี่คือหัวใจของการนำโมเดลไปใช้งาน เราจะโหลดไฟล์ .joblib ที่เราบันทึกไว้
# ซึ่งภายในไฟล์นั้นมีทั้ง "ตัวแปลงข้อความ (TF-IDF)" และ "โมเดล (RandomForest)"
# ที่พร้อมใช้งานคู่กัน

# --- กำหนดชื่อไฟล์และ Label ---
pipeline_filename = 'web_attack_pipeline.joblib'
label_names = {
    0: 'Normal',
    1: 'XSS',
    2: 'SQL Injection'
}

# --- โหลด Pipeline จากไฟล์ ---
print("="*50)
print(f"พยายามโหลด Pipeline จากไฟล์: '{pipeline_filename}'...")

# ตรวจสอบว่าไฟล์โมเดลมีอยู่จริงหรือไม่ ก่อนที่จะโหลด
if os.path.exists(pipeline_filename):
    loaded_pipeline = joblib.load(pipeline_filename)
    print("✅ โหลด Pipeline สำเร็จ! ตอนนี้พร้อมใช้งานแล้ว")
else:
    print(f"❌ ไม่พบไฟล์ Pipeline '{pipeline_filename}'!")
    print("กรุณาตรวจสอบว่าคุณได้อัปโหลดไฟล์เข้ามาใน Colab session นี้แล้ว")
    raise FileNotFoundError("ไม่พบไฟล์ Pipeline กรุณาอัปโหลดก่อนรันอีกครั้ง")

print("="*50)


# ==============================================================================
# ส่วนที่ 3: สร้างฟังก์ชันสำหรับทำนายข้อมูลใหม่
# ==============================================================================
# คำอธิบาย:
# เราจะสร้างฟังก์ชันที่รับ "ประโยค" ที่ต้องการทดสอบเข้ามา
# จากนั้นฟังก์ชันจะส่งประโยคนี้เข้าไปใน Pipeline ที่โหลดมา
# Pipeline จะจัดการแปลงข้อความเป็นตัวเลขและทำนายผลให้โดยอัตโนมัติ

def predict_attack(sentence: str):
    """
    ฟังก์ชันสำหรับทำนายประเภทของ Web Request โดยใช้ Pipeline ที่โหลดมา
    """
    print(f"\nกำลังวิเคราะห์: '{sentence}'")

    # ใช้ Pipeline ทำนายผลโดยตรงจากข้อความดิบ
    # ไม่ต้องทำ Feature Engineering เอง เพราะทุกอย่างอยู่ใน Pipeline แล้ว
    prediction_code = loaded_pipeline.predict([sentence]) # ต้องส่งเป็น list

    # แปลงผลลัพธ์ตัวเลขกลับเป็นชื่อที่เข้าใจง่าย
    predicted_label_name = label_names.get(prediction_code[0], "ไม่ทราบประเภท")

    # แสดงผล
    print(f"--> 🎯 ผลการทำนาย: {predicted_label_name}")
    return predicted_label_name

# ==============================================================================
# ส่วนที่ 4: ทดลองใช้งานฟังก์ชันกับข้อมูลตัวอย่าง (ฉบับแก้ไข)
# ==============================================================================
print("\n\n" + "="*50)
print("🚀 เริ่มทดสอบการทำนายผลกับข้อมูลใหม่ 🚀")
print("="*50)

# --- ตัวอย่างที่ 1: การใช้งานปกติ ---
# รูปแบบนี้ถูกต้องแล้ว เพราะเหมือน request ทั่วไป
predict_attack("id=3&nombre=Vino+Rioja&precio=100&cantidad=25&B1=A�adir+al+carrito")

# --- ตัวอย่างที่ 2: การโจมตีแบบ SQL Injection (แก้ไข) ---
# ห่อหุ้ม payload การโจมตีด้วยรูปแบบของ URL Query String
predict_attack("GET /products/show.php?id=105' UNION SELECT user, password FROM users-- HTTP/1.1")

# --- ตัวอย่างที่ 3: การโจมตีแบบ Cross-Site Scripting (XSS) (แก้ไข) ---
# ใส่เข้าไปใน URL ที่สมมติขึ้น
predict_attack("GET /search.php?q=<script>alert('hacked')</script> HTTP/1.1")

# --- ตัวอย่างที่ 4: การโจมตีที่มีความซับซ้อน (แก้ไข) ---
# รูปแบบนี้ค่อนข้างสมบูรณ์ แต่อาจเพิ่ม Method และ Protocol เพื่อให้ชัดเจนขึ้น
predict_attack("POST /api/orders query='SELECT * FROM Orders WHERE ShipCountry='Germany' AND (OrderID=10248 OR OrderID=10249)' HTTP/1.1")

print("\n" + "="*50)
print("การทดสอบเสร็จสิ้น")
print("="*50)